import logging
# logging.basicConfig(level=logging.DEBUG)

import oci
import time
import json
# from oci.ai_speech import AIServiceSpeechClient
# from oci.ai_speech.models import (
#     CreateTranscriptionJobDetails,
#     ObjectListFileInputLocation,
#     ObjectLocation,
#     TranscriptionModelDetails
# )

# === åƒæ•¸è¨­å®š ===
AUDIO_FILE = "sample_mixed.wav"
BUCKET_NAME = "Speech"
OBJECT_NAME = "uploaded-sample.wav"
NAMESPACE = "" #enter your upload bucket namespace
input_prefix = "uploads"
output_prefix = "speech-output"
final_prefix = "transcripts"

# Speech configuration
model_type = "ORACLE"  # or "WHISPER_MEDIUM" or "ORACLE"
language_code = "en"  # "en-US" for ORACLE "en" for whisper

# === åˆå§‹åŒ–å®¢æˆ¶ç«¯ ===
config = oci.config.from_file()
config["region"] = "us-chicago-1"
object_storage = oci.object_storage.ObjectStorageClient(config)
object_storage_client = oci.object_storage.ObjectStorageClient(config)
ai_speech_client = oci.ai_speech.AIServiceSpeechClient(config)
COMPARTMENT_ID = "" #enter your COMPARTMENT_ID

def upload_audio_file(file_name):
    """Uploads an audio file to Object Storage."""
    with open(file_name, "rb") as f:
        audio_upload_name = input_prefix + "/" + OBJECT_NAME
        object_storage_client.put_object(
            namespace_name=NAMESPACE,
            bucket_name=BUCKET_NAME,
            object_name=audio_upload_name,
            put_object_body=f,  # Use read() to get the file content
        )
    print("Successfully uploaded input file to Object Storage")
    return audio_upload_name

from pydub import AudioSegment
import os
def boost_upload_audio_file(file_name):
    """
    å£“ç¸®éŸ³æª”ç‚º FLAC å¾Œä¸Šå‚³è‡³ OCI Object Storageã€‚
    :param file_name: åŸå§‹ .wav æª”æ¡ˆè·¯å¾‘
    :return: ä¸Šå‚³å¾Œçš„ Object åç¨±
    """
    # ç¢ºä¿è½‰æˆ FLAC å­˜åœ¨
    file_root = os.path.splitext(os.path.basename(file_name))[0]
    compressed_file = f"{file_root}.flac"
    audio = AudioSegment.from_wav(file_name)
    audio.export(compressed_file, format="flac")

    # æº–å‚™ä¸Šå‚³
    object_name = f"{input_prefix}/{compressed_file}"
    with open(compressed_file, "rb") as f:
        object_storage_client.put_object(
            namespace_name=NAMESPACE,
            bucket_name=BUCKET_NAME,
            object_name=object_name,
            put_object_body=f
        )

    print(f"âœ… å·²ä¸Šå‚³ FLAC æª”æ¡ˆï¼š{object_name}")
    os.remove(compressed_file)  # ä¸Šå‚³å®Œåˆªé™¤æš«å­˜æª”

    return object_name

def create_speech_job(audio_upload_name): 
    """Creates a speech transcription job."""
    print("Starting transcription process")  
    create_transcription_job_response = ai_speech_client.create_transcription_job(
        create_transcription_job_details=oci.ai_speech.models.CreateTranscriptionJobDetails(
            compartment_id=COMPARTMENT_ID,
            input_location=oci.ai_speech.models.ObjectListInlineInputLocation(
                location_type="OBJECT_LIST_INLINE_INPUT_LOCATION",
                object_locations=[
                    oci.ai_speech.models.ObjectLocation(
                        namespace_name=NAMESPACE,
                        bucket_name=BUCKET_NAME,
                        object_names=[audio_upload_name],
                    )
                ],
            ),
            output_location=oci.ai_speech.models.OutputLocation(
                namespace_name=NAMESPACE,
                bucket_name=BUCKET_NAME,
                prefix=output_prefix,
            ),
            #display_name=filename,
            model_details=oci.ai_speech.models.TranscriptionModelDetails(
                domain="GENERIC",
                model_type=model_type,
                # language_code=language_code,
                transcription_settings=oci.ai_speech.models.TranscriptionSettings(
                    diarization=oci.ai_speech.models.Diarization(
                        is_diarization_enabled=True
                    )
                ),
            ),
        )
    )

    job_id = create_transcription_job_response.data.id
    out_loc = create_transcription_job_response.data.output_location.prefix
    print(f"{job_id}, {out_loc}")
    return job_id, out_loc

def wait_for_job_completion(job_id, out_loc):
    """
    Waits for the Oracle STT transcription job to complete.
    Polls every 3 seconds and prints elapsed time.
    """
    start_time = time.time()
    poll_interval = 2  # æ¯æ¬¡è¼ªè©¢é–“éš”ç§’æ•¸

    while True:
        try:
            get_transcription_job_response = ai_speech_client.get_transcription_job(
                transcription_job_id=job_id
            )
            status = get_transcription_job_response.data.lifecycle_state

            elapsed = time.time() - start_time
            minutes, seconds = divmod(int(elapsed), 60)
            print(f"â±ï¸ ç­‰å¾…ä¸­... {status} | Elapsed: {minutes:02d}:{seconds:02d}")

            if status == "SUCCEEDED":
                print(f"âœ… Transcription completed in {minutes:02d}:{seconds:02d}")
                break
            elif status in ["FAILED", "CANCELED"]:
                raise RuntimeError(f"âŒ Job {status}. Please check OCI console.")

            time.sleep(poll_interval)

        except oci.exceptions.ServiceError as e:
            print(f"âš ï¸ OCI API Error: {e}")
            break

    # çµ„åˆè¼¸å‡º JSON çš„ object åç¨±
    ori_name = get_transcription_job_response.data.input_location.object_locations[0].object_names[0]
    res_file = f"{out_loc}{NAMESPACE}_{BUCKET_NAME}_{ori_name}.json"
    print(f"ğŸ“„ Output JSON file: {res_file}")
    return res_file


def download_speech_json(res_file):
    """Downloads the transcript from Object Storage."""
    print("Fetching speech json output")
    get_object_response = object_storage_client.get_object(
        namespace_name=NAMESPACE,
        bucket_name=BUCKET_NAME,
        http_response_content_type="text/plain",
        object_name=res_file,
    )
    json_data = json.loads(get_object_response.data.content)
    print("Get JSON from Object Storage, OK")
  
    return json_data

def create_transcript(json_data):
    """Creates a formatted transcript from JSON data."""
    print("Formatting transcript from raw json")
    transcript = ""
    # Check if 'transcriptions' exists in json_data
    if isinstance(json_data.get("transcriptions"), list) and len(json_data["transcriptions"]) > 0:
        for entry in json_data["transcriptions"][0]["tokens"]:
            text = entry["token"]
            # if current_speaker != speaker_index:
            #     speaker_name = speaker1_name if speaker_index == 0 else speaker2_name
            #     transcript += f"\n\n{speaker_name}: {text} "
            #     current_speaker = speaker_index
            # else:
            transcript += f"{text} "
    return transcript

def cleanup_all_job_output(prefix="speech-output/job-"):
    print("ğŸ§¹ é–‹å§‹æ¸…é™¤æ‰€æœ‰ job è¼¸å‡ºè³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ...")

    list_response = object_storage_client.list_objects(
        namespace_name=NAMESPACE,
        bucket_name=BUCKET_NAME,
        prefix=prefix,
        fields=["name"]
    )

    for obj in list_response.data.objects:
        obj_name = obj.name
        if obj_name.endswith("/"):
            continue  # è·³éè³‡æ–™å¤¾è™›æ“¬æ¨™è¨˜

        print(f"ğŸ—‘ï¸ åˆªé™¤æª”æ¡ˆ: {obj_name}")
        object_storage_client.delete_object(
            namespace_name=NAMESPACE,
            bucket_name=BUCKET_NAME,
            object_name=obj_name
        )

    print("âœ… å·²æ¸…é™¤æ‰€æœ‰ job-* è³‡æ–™å¤¾å…§çš„æª”æ¡ˆã€‚")

import whisper

def detect_language_from_wav(file_path: str, model_size: str = "tiny") -> str:
    """
    ä½¿ç”¨ OpenAI Whisper æœ¬åœ°æ¨¡å‹å¿«é€Ÿåˆ¤æ–· WAV éŸ³æª”çš„èªè¨€ã€‚
    
    :param file_path: éŸ³æª”è·¯å¾‘ï¼ˆ.wavï¼‰
    :param model_size: Whisper æ¨¡å‹å¤§å°ï¼Œå¯é¸ tinyã€baseã€smallï¼ˆè¶Šå°è¶Šå¿«ï¼‰
    :return: èªè¨€ä»£ç¢¼ï¼Œä¾‹å¦‚ 'en', 'zh', 'ja'
    """
    model = whisper.load_model(model_size)
    audio = whisper.load_audio(file_path)
    audio = whisper.pad_or_trim(audio)  # é•·åº¦å›ºå®šåŒ–

    mel = whisper.log_mel_spectrogram(audio).to(model.device)
    _, probs = model.detect_language(mel)

    # æ‰¾å‡ºæœ€é«˜æ©Ÿç‡çš„èªè¨€ä»£ç¢¼
    detected_lang = max(probs, key=probs.get)
    # print("ğŸŒ èªè¨€æ©Ÿç‡åˆ†å¸ƒï¼š")
    # for lang, p in sorted(probs.items(), key=lambda x: -x[1]):
    #     print(f"  {lang}: {p:.2%}")
        
    return detected_lang

def main(file_name):
    # cleanup_all_job_output()
    audio_upload_name = boost_upload_audio_file(file_name)
    # lang_code = detect_language_from_wav(AUDIO_FILE)
    # print("ğŸ¯ åµæ¸¬åˆ°çš„èªè¨€ï¼š", lang_code)
    # language_code = lang_code

    job_id, out_loc = create_speech_job(audio_upload_name)
    res_file = wait_for_job_completion(job_id, out_loc)
    json_data = download_speech_json(res_file)
    transcript = create_transcript(json_data)
    print(transcript)
    return transcript